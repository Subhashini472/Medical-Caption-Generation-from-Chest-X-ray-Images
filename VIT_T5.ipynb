{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c965fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, ViTModel\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586de34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preparation\n",
    "df_projections = pd.read_csv('content/Projection.csv')\n",
    "df_reports = pd.read_csv('content/report.csv')\n",
    "df = df_projections.merge(df_reports, on=\"uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e22b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_caption(row):\n",
    "    findings = row[\"findings\"]\n",
    "    impression = row[\"impression\"]\n",
    "    if pd.notna(findings) and pd.notna(impression):\n",
    "        return f\"{findings.strip()} {impression.strip()}\"\n",
    "    elif pd.notna(findings):\n",
    "        return findings.strip()\n",
    "    elif pd.notna(impression):\n",
    "        return impression.strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df[\"caption\"] = df.apply(clean_caption, axis=1)\n",
    "df = df.dropna(subset=[\"caption\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Encoder\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, images):\n",
    "        outputs = self.vit(pixel_values=images)\n",
    "        return outputs.last_hidden_state  # shape: (B, N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QFormer blocks\n",
    "class QFormerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super(QFormerBlock, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, queries, image_features):\n",
    "        # Self-Attention\n",
    "        q = self.norm1(queries)\n",
    "        q, _ = self.self_attn(q, q, q)\n",
    "\n",
    "        # Cross-Attention with image\n",
    "        q = self.norm2(q)\n",
    "        q, _ = self.cross_attn(q, image_features, image_features)\n",
    "\n",
    "        # Feedforward\n",
    "        q = self.norm3(q)\n",
    "        return self.feed_forward(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ddf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFormer(nn.Module):\n",
    "    def __init__(self, num_queries=32, dim=768, num_blocks=6, num_heads=12):\n",
    "        super(QFormer, self).__init__()\n",
    "        self.learned_queries = nn.Parameter(torch.randn(1, num_queries, dim))\n",
    "        self.blocks = nn.ModuleList([QFormerBlock(dim, num_heads) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, image_features):\n",
    "        B = image_features.size(0)\n",
    "        queries = self.learned_queries.expand(B, -1, -1)  # (B, num_queries, dim)\n",
    "        for block in self.blocks:\n",
    "            queries = block(queries, image_features)\n",
    "        return queries  # Output features for captioning or matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ab358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed T5 Integration\n",
    "class MedicalCaptioningModel(nn.Module):\n",
    "    def __init__(self, t5_model_name=\"t5-base\"):\n",
    "        super().__init__()\n",
    "        # Image encoder\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        \n",
    "        # QFormer for bridging vision and language\n",
    "        self.qformer = QFormer(num_queries=32, dim=768)\n",
    "        \n",
    "        # T5 model\n",
    "        self.t5 = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "        \n",
    "        # Projection from vision to T5 hidden dimension\n",
    "        self.vision_proj = nn.Linear(768, self.t5.config.d_model)\n",
    "        \n",
    "        # Freezing T5 encoder as we'll use our visual encoder instead\n",
    "        for param in self.t5.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, pixel_values, labels=None, decoder_attention_mask=None):\n",
    "        batch_size = pixel_values.size(0)\n",
    "        \n",
    "        # Extract image features\n",
    "        image_features = self.image_encoder(pixel_values)\n",
    "        \n",
    "        # Process through QFormer\n",
    "        qformer_output = self.qformer(image_features)\n",
    "        \n",
    "        # Project to T5 dimension\n",
    "        encoder_hidden_states = self.vision_proj(qformer_output)\n",
    "        \n",
    "        # Prepare hidden states in the format T5 expects\n",
    "        # Create a BaseModelOutput object for encoder_outputs\n",
    "        from transformers.modeling_outputs import BaseModelOutput\n",
    "        encoder_outputs = BaseModelOutput(\n",
    "            last_hidden_state=encoder_hidden_states,\n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "        \n",
    "        # Prepare decoder input ids (right-shifted labels)\n",
    "        if labels is not None:\n",
    "            decoder_input_ids = self.prepare_decoder_input_ids_from_labels(labels)\n",
    "        else:\n",
    "            # For inference, start with the pad token\n",
    "            decoder_input_ids = torch.full(\n",
    "                (batch_size, 1), \n",
    "                self.t5.config.pad_token_id, \n",
    "                dtype=torch.long, \n",
    "                device=encoder_hidden_states.device\n",
    "            )\n",
    "            \n",
    "        # Forward through T5 decoder - FIXED: removed encoder_attention_mask\n",
    "        outputs = self.t5(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "            \n",
    "        return outputs\n",
    "        \n",
    "    def prepare_decoder_input_ids_from_labels(self, labels):\n",
    "        \"\"\"Shift labels to create decoder input ids\"\"\"\n",
    "        decoder_input_ids = labels.clone()\n",
    "        \n",
    "        # Replace -100 with pad token id\n",
    "        decoder_input_ids[decoder_input_ids == -100] = self.t5.config.pad_token_id\n",
    "        \n",
    "        # Shift right\n",
    "        decoder_input_ids = torch.cat(\n",
    "            [\n",
    "                torch.full((decoder_input_ids.shape[0], 1), self.t5.config.decoder_start_token_id, \n",
    "                          device=decoder_input_ids.device),\n",
    "                decoder_input_ids[:, :-1]\n",
    "            ], \n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        return decoder_input_ids\n",
    "        \n",
    "    def generate(self, pixel_values, max_length=128, num_beams=4):\n",
    "        \"\"\"Generate captions for images\"\"\"\n",
    "        batch_size = pixel_values.size(0)\n",
    "        \n",
    "        # Extract image features\n",
    "        image_features = self.image_encoder(pixel_values)\n",
    "        \n",
    "        # Process through QFormer\n",
    "        qformer_output = self.qformer(image_features)\n",
    "        \n",
    "        # Project to T5 dimension\n",
    "        encoder_hidden_states = self.vision_proj(qformer_output)\n",
    "        \n",
    "        # Create a BaseModelOutput object for encoder_outputs\n",
    "        from transformers.modeling_outputs import BaseModelOutput\n",
    "        encoder_outputs = BaseModelOutput(\n",
    "            last_hidden_state=encoder_hidden_states,\n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "        \n",
    "        # Generate using T5's generation method\n",
    "        output_ids = self.t5.generate(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        return output_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for T5\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_root_dir, tokenizer, transform=None, image_size=224, max_length=128):\n",
    "        self.df = dataframe\n",
    "        self.image_root_dir = image_root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "        ]) if transform is None else transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.image_root_dir, row['filename'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        caption = row['caption']\n",
    "        \n",
    "        # Tokenize caption for T5\n",
    "        encoding = self.tokenizer(\n",
    "            caption, \n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Extract and prepare labels for training\n",
    "        labels = encoding.input_ids.clone().squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": encoding.attention_mask.squeeze(0)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf4bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training and evaluation code\n",
    "image_root = \"C:/Users/Administrator//Desktop/125150051/Data/images/image_normalized/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Split dataset\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenizer and transform\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and Dataloader\n",
    "train_dataset = ChestXrayDataset(train_df, image_root_dir=image_root, tokenizer=tokenizer, transform=image_transform)\n",
    "test_dataset = ChestXrayDataset(test_df, image_root_dir=image_root, tokenizer=tokenizer, transform=image_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ae285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 2.2599\n",
      "Epoch 2 - Training Loss: 1.2731\n",
      "Epoch 3 - Training Loss: 1.0077\n",
      "Epoch 4 - Training Loss: 0.8016\n",
      "Epoch 5 - Training Loss: 0.6172\n",
      "Epoch 6 - Training Loss: 0.4661\n",
      "Epoch 7 - Training Loss: 0.3597\n",
      "Epoch 8 - Training Loss: 0.2891\n",
      "Epoch 9 - Training Loss: 0.2494\n",
      "Epoch 10 - Training Loss: 0.2264\n",
      "Epoch 11 - Training Loss: 0.2119\n",
      "Epoch 12 - Training Loss: 0.2006\n",
      "Epoch 13 - Training Loss: 0.1915\n",
      "Epoch 14 - Training Loss: 0.1869\n",
      "Epoch 15 - Training Loss: 0.1786\n",
      "Epoch 16 - Training Loss: 0.1752\n",
      "Epoch 17 - Training Loss: 0.1713\n",
      "Epoch 18 - Training Loss: 0.1636\n",
      "Epoch 19 - Training Loss: 0.1597\n",
      "Epoch 20 - Training Loss: 0.1586\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = MedicalCaptioningModel(t5_model_name=\"t5-base\").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in progress_bar:\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            pixel_values=pixel_values,\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feedf0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Predictions after Epoch 20\n",
      "Image: 632_IM-2213-1002.dcm.png\n",
      "GT     : Cardiac and mediastinal silhouette are unremarkable. Lungs are clear. No focal consolidation, pneumothorax, or pleural effusion identified. XXXX and soft tissue are unremarkable. No acute cardiopulmonary abnormality.\n",
      "Predicted: the heart size is normal. the mediastinal contour is within normal limits. the lungs are free of any focal infiltrates. there are no nodules or masses. no visible pneumothorax. no visible pleural fluid\n",
      "--------------------------------------------------\n",
      "Image: 1509_IM-0331-2001.dcm.png\n",
      "GT     : Lungs are clear bilaterally.There is no focal consolidation, pleural effusion, or pneumothoraces. Cardiomediastinal silhouette is within normal limits. XXXX are unremarkable. No acute cardiopulmonary abnormality.\n",
      "Predicted: the heart size and mediastinal silhouette are within normal limits for contour. the lungs are clear. no pneumothorax or pleural effusions. the xxxx are intact. stable left basilar atelectasis\n",
      "--------------------------------------------------\n",
      "Image: 53_IM-2138-1001.dcm.png\n",
      "GT     : There extremely low lung volumes. there is right basilar opacity. There is no pneumothorax. There is no large pleural effusion. Cardiac silhouette and mediastinal contours are within normal limits. Low lung volumes with right basilar atelectasis. Otherwise, no acute cardiopulmonary disease.\n",
      "Predicted: the heart size is upper limits of normal. the pulmonary xxxx and mediastinum are within normal limits. there is no pleural effusion or pneumothorax. there is mild streaky perihilar opac\n",
      "--------------------------------------------------\n",
      "Image: 3009_IM-1389-1001.dcm.png\n",
      "GT     : No focal areas of consolidation. No pleural effusions. No pneumothorax. Degenerative changes thoracic spine. Heart size normal limits. Cholecystectomy clips. No acute cardiopulmonary abnormality. .\n",
      "Predicted: the heart size and pulmonary vascularity appear within normal limits. the lungs are free of focal airspace disease. no pleural effusion or pneumothorax is seen. no evidence of active disease.\n",
      "--------------------------------------------------\n",
      "Image: 3502_IM-1707-1001.dcm.png\n",
      "GT     : There has been interval performance of CABG with multiple XXXX sternotomy XXXX, surgical clips, and CABG markers. All of the XXXX sternotomy XXXX are broken, and a fragment at a sternotomy XXXX appears to XXXX within the left posterior pleural space. Stable cardiomegaly and central pulmonary vascular prominence. No focal consolidation, pneumothorax, or effusion. Relative elevation of the left hemidiaphragm noted. No acute bony abnormality. Cardiomegaly with surgical changes of CABG, with numerous broken XXXX sternotomy XXXX and a sternotomy XXXX fragment noted XXXX in the posterior left pleural space.\n",
      "Predicted: the heart size is within normal limits. cardiomediastinal contour is normal. there is a right upper lobe nodule measuring 8 mm in diameter. trachea is midline. the lungs otherwise clear. xxxx and soft tissues\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Function to generate caption\n",
    "def generate_caption(model, tokenizer, image):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Prepare image\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = image_transform(image).unsqueeze(0).to(device)\n",
    "        else:\n",
    "            image = image.unsqueeze(0).to(device) if image.dim() == 3 else image.to(device)\n",
    "            \n",
    "        # Generate tokens\n",
    "        output_ids = model.generate(image)\n",
    "        \n",
    "        # Decode to text\n",
    "        caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        return caption\n",
    "\n",
    "# Save model function\n",
    "def save_model(model, tokenizer, save_path):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, \"model_weights.pth\"))\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Load model function\n",
    "def load_model(model_path, tokenizer_path, t5_model_name=\"t5-base\"):\n",
    "    model = MedicalCaptioningModel(t5_model_name).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(model_path, \"model_weights.pth\"), map_location=device))\n",
    "    tokenizer = T5Tokenizer.from_pretrained(tokenizer_path)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "# Inference on Test Set\n",
    "print(\"\\n--- Predictions on Test Set ---\")\n",
    "for i in range(5):\n",
    "    image_path = os.path.join(image_root, test_df.iloc[i]['filename'])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    input_tensor = image_transform(image).to(device)\n",
    "\n",
    "    prediction = generate_caption(model, tokenizer, input_tensor)\n",
    "    print(f\"Image: {test_df.iloc[i]['filename']}\")\n",
    "    print(f\"GT     : {test_df.iloc[i]['caption']}\")\n",
    "    print(f\"Predicted: {prediction}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Save the trained model\n",
    "save_path = \"t5_medical_image_caption_model\"\n",
    "save_model(model, tokenizer, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d533b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using updated evaluate_bleu() function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1486/1486 [34:41<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average BLEU-1: 0.5268\n",
      "Average BLEU-2: 0.4236\n",
      "Average BLEU-3: 0.3472\n",
      "Average BLEU-4: 0.2143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# BLEU Evaluation\n",
    "def evaluate_bleu(model, tokenizer, test_df, image_root_dir, transform, max_samples=None): \n",
    "    print(\"✅ Using BLEU evaluation function for T5 model\")\n",
    "    \n",
    "    bleu1_scores, bleu2_scores, bleu3_scores, bleu4_scores = [], [], [], []\n",
    "    model.eval()\n",
    "    smooth = SmoothingFunction().method1\n",
    "\n",
    "    total_samples = len(test_df) if max_samples is None else min(max_samples, len(test_df))\n",
    "\n",
    "    for i in tqdm(range(total_samples)):\n",
    "        row = test_df.iloc[i]\n",
    "        image_path = os.path.join(image_root_dir, row['filename'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = transform(image).to(device)\n",
    "\n",
    "        generated = generate_caption(model, tokenizer, image)\n",
    "        reference = row['caption']\n",
    "\n",
    "        ref_tokens = reference.lower().split()\n",
    "        gen_tokens = generated.lower().split()\n",
    "\n",
    "        bleu1 = sentence_bleu([ref_tokens], gen_tokens, weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
    "        bleu2 = sentence_bleu([ref_tokens], gen_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth)\n",
    "        bleu3 = sentence_bleu([ref_tokens], gen_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth)\n",
    "        bleu4 = sentence_bleu([ref_tokens], gen_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
    "\n",
    "        bleu1_scores.append(bleu1)\n",
    "        bleu2_scores.append(bleu2)\n",
    "        bleu3_scores.append(bleu3)\n",
    "        bleu4_scores.append(bleu4)\n",
    "\n",
    "    print(f\"\\nAverage BLEU-1: {sum(bleu1_scores)/len(bleu1_scores):.4f}\")\n",
    "    print(f\"Average BLEU-2: {sum(bleu2_scores)/len(bleu2_scores):.4f}\")\n",
    "    print(f\"Average BLEU-3: {sum(bleu3_scores)/len(bleu3_scores):.4f}\")\n",
    "    print(f\"Average BLEU-4: {sum(bleu4_scores)/len(bleu4_scores):.4f}\")\n",
    "\n",
    "# Run BLEU evaluation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "evaluate_bleu(model, tokenizer, test_df, image_root, transform=test_transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_gpu)",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
